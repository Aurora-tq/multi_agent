#å•çº¿ä»£ç 
import json
import os
import re
from typing import List, Dict
from pydantic import Field
from app.tool.base import BaseTool, ToolResult
from app.llm import LLM
from app.logger import logger

# RAG ä¸ Re-rank ä¾èµ–
try:
    from langchain_community.vectorstores import FAISS
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    from sentence_transformers import CrossEncoder
except ImportError:
    raise ImportError("Please install dependencies: pip install langchain-huggingface faiss-cpu sentence-transformers")

# å…¨å±€æ¨¡å‹ (å•ä¾‹æ¨¡å¼)
_EMBEDDING_MODEL = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
_RERANK_MODEL = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")


class StructuredRetrievalTool(BaseTool):
    name: str = "structured_retrieval"
    description: str = """
    Advanced batch-processing extraction tool. Processes multiple files,
    extracts raw structured insights, and performs re-ranking.
    Returns RAW markdown content without cleaning.
    """

    parameters: dict = {
        "type": "object",
        "properties": {
            "file_paths": {
                "type": "array",
                "items": {"type": "string"},
                "description": "List of local paths to the files (.md)."
            },
            "query": {
                "type": "string",
                "description": "The specific extraction query (e.g. 'smart home trends')."
            },
            "source_url": {
                "type": "string",
                "description": "Optional global source URL (for plain markdown files)."
            }
        },
        "required": ["file_paths", "query"]
    }

    llm: LLM = Field(default_factory=LLM, exclude=True)

    async def execute(
        self,
        query: str,
        file_paths: List[str] = None,
        source_url: str = None,
        **kwargs
    ) -> ToolResult:
        logger.info(f"ğŸ” Starting Raw Structured Extraction for query: '{query}'")

        session_id = os.environ.get("MANUS_SESSION_ID", "default_session")
        all_docs_pool: List[Document] = []
        execution_summary = []

        # 1) æ‰¹é‡åŠ è½½ï¼šä»…å¤„ç† Markdown æ–‡ä»¶
        for path in file_paths:
            file_name = os.path.basename(path)
            try:
                if not os.path.exists(path):
                    continue

                # åªæ¥å— markdownï¼ˆä½ ä¹Ÿå¯ä»¥æ”¾å®½åˆ° .md/.markdownï¼‰
                if not (path.endswith(".md") or path.endswith(".markdown")):
                    logger.warning(f"âš ï¸ Skip non-markdown file: {file_name}")
                    continue

                with open(path, "r", encoding="utf-8") as f:
                    content = f.read()

                logger.info(f"ğŸ“ Processing Markdown (Raw): {file_name}")

                # 1) ä¼˜å…ˆä½¿ç”¨å‚æ•°ä¼ å…¥çš„ source_url
                current_source = source_url

                # 2) å¦‚æœæ²¡ä¼  source_urlï¼Œå†å°è¯•ä» HTML æ³¨é‡Šä¸­æå–
                if not current_source:
                    source_match = re.search(r"<!--\s*Source:\s*(.*?)\s*-->", content)
                    if source_match:
                        current_source = source_match.group(1).strip()

                # 3) å¦‚æœä»ç„¶æ²¡æœ‰ sourceï¼Œå…œåº•
                if not current_source:
                    current_source = "unknown_source"
                    logger.warning(f"âš ï¸ [Markdown] No source URL found for {file_name}, using 'unknown_source'")

                # 4) æŒ‰æ ‡é¢˜åˆ‡åˆ†å¹¶å…¥æ± 
                sections = self._split_markdown_by_headers(
                    content,
                    min_level=1,
                    max_level=3,
                    max_chars=2000,
                    overlap=150
                )

                md_docs: List[Document] = []
                for sec in sections:
                    d = Document(page_content=sec, metadata={})
                    d.metadata["source_url"] = current_source
                    d.metadata["file_type"] = "markdown"
                    d.metadata["file_name"] = file_name
                    md_docs.append(d)

                all_docs_pool.extend(md_docs)
                execution_summary.append(f"âœ… [Markdown] {file_name} ({len(md_docs)} header sections)")

            except Exception as e:
                logger.error(f"Failed to process {file_name}: {e}")

        # 2) å‘é‡å¬å› + ç»“æ„åŒ–æå– (Raw Mode)
        final_data = []
        if all_docs_pool:
            logger.info(f"ğŸ“Š Global Pool: {len(all_docs_pool)} chunks. Vector Search...")

            vector_store = FAISS.from_documents(all_docs_pool, _EMBEDDING_MODEL)
            retrieved_candidates = vector_store.similarity_search(query, k=200)

            docs_info = []
            for d in retrieved_candidates:
                docs_info.append({
                    "content": d.page_content,
                    "source_url": d.metadata.get("source_url"),
                    "file_type": d.metadata.get("file_type", "unknown")
                })

            structured_items = self._process_structured_content_raw(docs_info)

            structured_docs_for_rerank = [
                Document(page_content=item["text"], metadata=item)
                for item in structured_items
            ]

            reranked_docs = self._perform_rerank(query, structured_docs_for_rerank, top_k=50)

            for doc in reranked_docs:
                final_data.append({
                    "text": doc.page_content,
                    "images": doc.metadata.get("images", []),
                    "source_url": doc.metadata.get("source_url")
                })

        # 3) ä¿å­˜
        if final_data:
            master_save_path = self._save_final_data(final_data, session_id)
            return ToolResult(output=f"Raw extraction complete. Saved {len(final_data)} items to: {master_save_path}")

        return ToolResult(output="Batch process finished, no items found.")

    def _process_structured_content_raw(self, docs_info: List[Dict]) -> List[Dict]:
        """
        [Raw Mode] ç»“æ„åŒ–å†…å®¹æå–
        - ä¸ç§»é™¤ Markdown ç¬¦å·
        - ä¸ç§»é™¤å›¾ç‰‡æ ‡ç­¾ (ä¿ç•™åœ¨ text ä¸­)
        - ä¾ç„¶æå– images åˆ—è¡¨ä¾›ä¸‹æ¸¸ä½¿ç”¨
        """
        extracted_items = []

        for info in docs_info:
            raw_content = info["content"]
            source = info.get("source_url", "unknown")
            file_type = info.get("file_type", "unknown")

            primary_segments = re.split(r"(?:\n|^)#{1,6}\s+|(?:\n|^)-{3,}(?:\n|$)", raw_content)

            final_segments = []
            for seg in primary_segments:
                if len(seg) > 800:
                    final_segments.extend(seg.split("\n\n"))
                else:
                    final_segments.append(seg)

            for section in final_segments:
                section = section.strip()
                if len(section) < 10:
                    continue

                image_urls = []
                image_urls.extend(re.findall(r"!\[.*?\]\((.*?)\)", section))
                # image_urls.extend(re.findall(r"<resource_info>(.*?)</resource_info>", section))
                unique_images = list(set(image_urls))

                extracted_items.append({
                    "text": section,
                    "images": unique_images,
                    "source_url": source,
                    "file_type": file_type
                })

        return extracted_items

    def _perform_rerank(self, query: str, docs: List["Document"], top_k: int) -> List["Document"]:
        """Cross-Encoder é‡æ’åº"""
        if not docs:
            return []
        unique_docs = {d.page_content: d for d in docs}.values()
        docs = list(unique_docs)

        pairs = [[query, doc.page_content] for doc in docs]
        scores = _RERANK_MODEL.predict(pairs)
        scored_docs = sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)
        return [doc for score, doc in scored_docs[:top_k]]

    def _save_final_data(self, new_data: List[Dict], session_id: str) -> str:
        save_dir = f"workspace/{session_id}/structured_data"
        os.makedirs(save_dir, exist_ok=True)
        path = os.path.join(save_dir, f"combined_data_{session_id}.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump({"data": new_data}, f, ensure_ascii=False, indent=4)
        return path
    def _split_markdown_by_headers(
        self,
        content: str,
        min_level: int = 1,
        max_level: int = 3,
        max_chars: int = 2000,
        overlap: int = 150,
        ) -> List[str]:
        """
        æŒ‰ Markdown ATX æ ‡é¢˜ (#, ##, ### ...) åˆ‡åˆ†ã€‚
        - min_level/max_level æ§åˆ¶ç”¨å“ªäº›æ ‡é¢˜ä½œä¸ºâ€œåˆ‡åˆ†ç‚¹â€
        - æ¯ä¸ª section å¦‚æœè¶…è¿‡ max_charsï¼Œä¼šå†åšäºŒæ¬¡åˆ‡åˆ†ï¼ˆå¸¦ overlapï¼‰
        """
        # åŒ¹é…è¡Œé¦–æ ‡é¢˜ï¼š# åˆ° ######ï¼Œåé¢è‡³å°‘ä¸€ä¸ªç©ºæ ¼
        header_re = re.compile(r'^(#{1,6})\s+(.+?)\s*$', re.MULTILINE)

        matches = list(header_re.finditer(content))
        if not matches:
            # æ²¡æ ‡é¢˜å°±æ•´ä½“è¿”å›ï¼Œåé¢äºŒæ¬¡åˆ‡åˆ†
            return self._chunk_text(content, max_chars=max_chars, overlap=overlap)

        sections: List[str] = []
        for i, m in enumerate(matches):
            level = len(m.group(1))
            # åªä»¥æŒ‡å®š level èŒƒå›´å†…çš„æ ‡é¢˜ä½œä¸ºåˆ‡åˆ†ç‚¹
            if not (min_level <= level <= max_level):
                continue

            start = m.start()
            # æ‰¾ä¸‹ä¸€ä¸ªâ€œå¯ç”¨æ ‡é¢˜åˆ‡åˆ†ç‚¹â€çš„ä½ç½®ä½œä¸º end
            end = len(content)
            for j in range(i + 1, len(matches)):
                lvl2 = len(matches[j].group(1))
                if min_level <= lvl2 <= max_level:
                    end = matches[j].start()
                    break

            block = content[start:end].strip()
            if block:
                sections.append(block)

        # å¦‚æœå› ä¸º level è¿‡æ»¤å¯¼è‡´ sections ä¸ºç©ºï¼Œé™çº§ï¼šæŒ‰ä»»æ„æ ‡é¢˜åˆ‡
        if not sections:
            sections = []
            for i, m in enumerate(matches):
                start = m.start()
                end = matches[i + 1].start() if i + 1 < len(matches) else len(content)
                block = content[start:end].strip()
                if block:
                    sections.append(block)

        # äºŒæ¬¡åˆ‡åˆ†ï¼šé¿å…æŸä¸ª section å¤ªé•¿
        final_sections: List[str] = []
        for s in sections:
            if len(s) > max_chars:
                final_sections.extend(self._chunk_text(s, max_chars=max_chars, overlap=overlap))
            else:
                final_sections.append(s)

        return final_sections


    def _chunk_text(self, text: str, max_chars: int = 2000, overlap: int = 150) -> List[str]:
        """ç®€å•æŒ‰å­—ç¬¦é•¿åº¦åˆ‡ chunkï¼ˆç”¨äº section å¤ªé•¿æ—¶çš„äºŒæ¬¡åˆ‡åˆ†ï¼‰"""
        text = text.strip()
        if len(text) <= max_chars:
            return [text] if text else []

        chunks = []
        start = 0
        while start < len(text):
            end = min(len(text), start + max_chars)
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            if end == len(text):
                break
            start = max(0, end - overlap)
        return chunks

#åŒçº¿ä»£ç 
# import json
# import os
# import re
# import asyncio
# from typing import List, Optional, Dict, Any
# from pydantic import Field
# from app.schema import Message 

# # å·¥å…·ç±»ä¾èµ–
# from app.tool.base import BaseTool, ToolResult
# from app.llm import LLM
# from app.logger import logger

# # RAG ä¸ Re-rank ä¾èµ–
# try:
#     from langchain_text_splitters import MarkdownTextSplitter, RecursiveCharacterTextSplitter
#     from langchain_community.vectorstores import FAISS
#     from langchain_huggingface import HuggingFaceEmbeddings
#     from langchain_core.documents import Document
#     from sentence_transformers import CrossEncoder
# except ImportError:
#     raise ImportError("Please install dependencies: pip install langchain-huggingface faiss-cpu sentence-transformers")

# # å…¨å±€æ¨¡å‹ (å•ä¾‹æ¨¡å¼)
# _EMBEDDING_MODEL = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
# _RERANK_MODEL = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

# class StructuredRetrievalTool(BaseTool):
#     name: str = "structured_retrieval"
#     description: str = """
#     Advanced batch-processing extraction tool. Processes multiple files, 
#     extracts raw structured insights, and performs re-ranking.
#     Returns RAW markdown content without cleaning.
#     """
    
#     parameters: dict = {
#         "type": "object",
#         "properties": {
#             "file_paths": {
#                 "type": "array",
#                 "items": {"type": "string"},
#                 "description": "List of local paths to the files (.md or extracted_context.txt)."
#             },
#             "query": {
#                 "type": "string",
#                 "description": "The specific extraction query (e.g. 'smart home trends')."
#             },
#             "source_url": {
#                 "type": "string",
#                 "description": "Optional global source URL (for plain markdown files)."
#             }
#         },
#         "required": ["file_paths", "query"]
#     }

#     llm: LLM = Field(default_factory=LLM, exclude=True)

#     async def execute(self, query: str, file_paths: List[str] = None, source_url: str = None, **kwargs) -> ToolResult:
#         logger.info(f"ğŸ” Starting Raw Structured Extraction for query: '{query}'")
        
#         session_id = os.environ.get("MANUS_SESSION_ID", "default_session")
#         all_docs_pool = [] 
#         execution_summary = []

#         # 1. æ‰¹é‡å¾ªç¯ï¼šåŠ è½½æ‰€æœ‰æ–‡ä»¶
#         for path in file_paths:
#             file_name = os.path.basename(path)
#             try:
#                 if not os.path.exists(path):
#                     continue
                
#                 with open(path, 'r', encoding='utf-8') as f:
#                     content = f.read()

#                 # --- åˆ†æ”¯ A: å¤„ç† ms-agent çš„ output txt ---
#                 if path.endswith('.txt') or 'extracted_context' in file_name:
        
#                     ms_docs = self._parse_ms_agent_file(content)
#                     all_docs_pool.extend(ms_docs)
#                     execution_summary.append(f"âœ… [Visual] {file_name} ({len(ms_docs)} chunks)")

#                 # --- åˆ†æ”¯ B: å¤„ç†æ™®é€š Markdown æ–‡ä»¶ ---
#                 else:
#                     logger.info(f"ğŸ“ Processing Markdown (Raw): {file_name}")
#                     print("Processing Markdown (Raw):", file_name)
#                     # 1) ä¼˜å…ˆä½¿ç”¨å‚æ•°ä¼ å…¥çš„ source_url
#                     current_source = source_url

#                     # 2) å¦‚æœæ²¡ä¼  source_urlï¼Œå†å°è¯•ä» HTML æ³¨é‡Šä¸­æå–
#                     if not current_source:
#                         source_match = re.search(r'<!--\s*Source:\s*(.*?)\s*-->', content)
#                         if source_match:
#                             current_source = source_match.group(1).strip()

#                     # 3) å¦‚æœä»ç„¶æ²¡æœ‰ sourceï¼Œå…œåº•
#                     if not current_source:
#                         current_source = "unknown_source"
#                         logger.warning(f"âš ï¸ [Markdown] No source URL found for {file_name}, using 'unknown_source'")

#                     # 4) âœ… æ— è®ºæœ‰æ²¡æœ‰ sourceï¼Œéƒ½è¦åˆ‡åˆ†å¹¶å…¥æ± 
#                     sections = self._split_markdown_by_headers(
#                         content,
#                         min_level=1,
#                         max_level=3,     # å¸¸è§ï¼šç”¨åˆ° ### å°±å¤Ÿäº†ï¼›ä½ ä¹Ÿå¯ä»¥æ”¹åˆ° 6
#                         max_chars=2000,  # æ¯ä¸ªæ ‡é¢˜å—æœ€å¤§é•¿åº¦ï¼Œè¶…å‡ºä¼šäºŒæ¬¡åˆ‡åˆ†
#                         overlap=150
#                     )

#                     md_docs = []
#                     for sec in sections:
#                         d = Document(page_content=sec, metadata={})
#                         d.metadata["source_url"] = current_source
#                         d.metadata["file_type"] = "markdown"
#                         d.metadata["file_name"] = file_name
#                         md_docs.append(d)

#                     all_docs_pool.extend(md_docs)
#                     execution_summary.append(f"âœ… [Markdown] {file_name} ({len(md_docs)} header sections)")
#                     # splitter = MarkdownTextSplitter(chunk_size=512, chunk_overlap=100)
#                     # md_docs = splitter.create_documents([content])

#                     # for doc in md_docs:
#                     #     doc.metadata["source_url"] = current_source
#                     #     doc.metadata["file_type"] = "markdown"
#                     #     doc.metadata["file_name"] = file_name  # å¯é€‰ï¼šæ–¹ä¾¿è¿½è¸ª

#                     # all_docs_pool.extend(md_docs)
#                     # execution_summary.append(f"âœ… [Markdown] {file_name} ({len(md_docs)} chunks)")

#             except Exception as e:
#                 logger.error(f"Failed to process {file_name}: {e}")

#         # 2. å‘é‡å¬å› + ç»“æ„åŒ–æå– (Raw Mode)
#         final_data = []
#         if all_docs_pool:
#             logger.info(f"ğŸ“Š Global Pool: {len(all_docs_pool)} chunks. Vector Search...")
            
#             vector_store = FAISS.from_documents(all_docs_pool, _EMBEDDING_MODEL)
#             retrieved_candidates = vector_store.similarity_search(query, k=200)
            
#             # å‡†å¤‡æ•°æ®
#             docs_info = []
#             for d in retrieved_candidates:
#                 docs_info.append({
#                     "content": d.page_content,
#                     "source_url": d.metadata.get("source_url"),
#                     "file_type": d.metadata.get("file_type", "unknown")
#                 })
            
#             # --- è°ƒç”¨ä¸åšæ¸…æ´—çš„å¤„ç†å‡½æ•° ---
#             structured_items = self._process_structured_content_raw(docs_info)
            
#             structured_docs_for_rerank = [
#                 Document(page_content=item["text"], metadata=item) 
#                 for item in structured_items
#             ]
            
#             # Re-rank (Re-rank æ¨¡å‹é€šå¸¸èƒ½ç†è§£ Markdown è¯­æ³•ï¼Œæ‰€ä»¥æ²¡é—®é¢˜)
#             reranked_docs = self._perform_rerank(query, structured_docs_for_rerank, top_k=80)
            
#             for doc in reranked_docs:
#                 final_data.append({
#                     "text": doc.page_content, # è¿™é‡Œæ˜¯åŒ…å« markdown çš„åŸå§‹å†…å®¹
#                     "images": doc.metadata.get("images", []),
#                     "source_url": doc.metadata.get("source_url")
#                 })

#         # 3. ä¿å­˜
#         if final_data:
#             master_save_path = self._save_final_data(final_data, session_id)
#             return ToolResult(output=f"Raw extraction complete. Saved {len(final_data)} items to: {master_save_path}")
        
#         return ToolResult(output="Batch process finished, no items found.")

#     def _parse_ms_agent_file(self, content: str) -> List[Document]:
#         """è§£æ ms-agent æ–‡ä»¶ (ä¿æŒåŸé€»è¾‘ï¼Œä¸»è¦æ˜¯ä¸ºäº†åˆ‡åˆ† chunk)"""
#         documents = []
#         raw_segments = re.split(r'\n={10,}\n', content)
#         splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

#         for segment in raw_segments:
#             segment = segment.strip()
#             if not segment: continue
            
#             source_url = "unknown_visual_source"
#             source_match = re.search(r'ğŸ“„ \[Source\]:\s*(.*?)\n', segment)
#             if source_match:
#                 source_url = source_match.group(1).strip()
            
#             # è¿™é‡Œæˆ‘ä»¬ç¨å¾®æ¸…ç†ä¸€ä¸‹ headerï¼Œå› ä¸ºå®ƒæ˜¯äººä¸ºåŠ çš„åˆ†å‰²çº¿ï¼Œä¸å±äºå†…å®¹
#             clean_content = re.sub(r'ğŸ“„ \[Source\]:.*?\n-+\n', '', segment, flags=re.DOTALL)
            
#             chunks = splitter.create_documents([clean_content])
            
#             for chunk in chunks:
#                 chunk.metadata["source_url"] = source_url
#                 chunk.metadata["file_type"] = "ms_agent_txt"
#                 documents.append(chunk)
#         return documents

#     def _process_structured_content_raw(self, docs_info: List[Dict]) -> List[Dict]:
#         """
#         [Raw Mode] ç»“æ„åŒ–å†…å®¹æå–
#         - ä¸ç§»é™¤ Markdown ç¬¦å·
#         - ä¸ç§»é™¤å›¾ç‰‡æ ‡ç­¾ (ä¿ç•™åœ¨ text ä¸­)
#         - ä¾ç„¶æå– images åˆ—è¡¨ä¾›ä¸‹æ¸¸ä½¿ç”¨
#         """
#         extracted_items = []
        
#         for info in docs_info:
#             raw_content = info["content"]
#             source = info.get("source_url", "unknown")
#             file_type = info.get("file_type", "unknown")
            
#             # 1. ä¾ç„¶åšåˆ†æ®µï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦ä»¥æ®µè½ä¸ºå•ä½è¿›è¡Œ Rerank
#             # ç®€å•çš„æŒ‰åŒæ¢è¡Œç¬¦åˆ‡åˆ†ï¼Œæˆ–è€…æŒ‰ Markdown æ ‡é¢˜åˆ‡åˆ†
#             primary_segments = re.split(r'(?:\n|^)#{1,6}\s+|(?:\n|^)-{3,}(?:\n|$)', raw_content)
            
#             final_segments = []
#             for seg in primary_segments:
#                 if len(seg) > 800:
#                     sub_segs = seg.split('\n\n')
#                     final_segments.extend(sub_segs)
#                 else:
#                     final_segments.append(seg)
            
#             # 2. é€æ®µå¤„ç†
#             for section in final_segments:
#                 section = section.strip()
#                 if len(section) < 10: continue 
                
#                 # A. æå–å›¾ç‰‡åˆ—è¡¨ (ä¸ºäº†æ–¹ä¾¿ä¸‹æ¸¸å·¥å…·ç´¢å¼•ï¼Œæå–æ­¥éª¤ä¸èƒ½çœ)
#                 image_urls = []
#                 md_imgs = re.findall(r'!\[.*?\]\((.*?)\)', section)
#                 image_urls.extend(md_imgs)
#                 res_imgs = re.findall(r'<resource_info>(.*?)</resource_info>', section)
#                 image_urls.extend(res_imgs)
                
#                 unique_images = list(set(image_urls))
                
#                 # B. ã€å…³é”®ã€‘ä¸åšæ¸…æ´—ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ–‡æœ¬
#                 # raw_text ä¿ç•™äº† ![img](url), [link](url), **bold**, ### header
#                 raw_text = section
#                 #B. æ¸…æ´—æ–‡æœ¬
#                 # clean_text = re.sub(r'!\[.*?\]\(.*?\)', '', section)
#                 # clean_text = re.sub(r'<resource_info>.*?</resource_info>', '', clean_text)
#                 # clean_text = re.sub(r'\[\d+\]', '', clean_text)
#                 # clean_text = re.sub(r'\[(.*?)\]\(.*?\)', r'\1', clean_text) # ç§»é™¤é“¾æ¥ä¿ç•™æ–‡å­—
#                 # clean_text = re.sub(r'\s+', ' ', clean_text).strip()
#                 # clean_text = re.sub(r'^[#\-\*]+\s*', '', clean_text) 
                
#                 # C. æ„å»ºç»“æœ
#                 extracted_items.append({
#                     "text": raw_text,         # åŸå§‹æ–‡æœ¬
#                     "images": unique_images,  # å›¾ç‰‡åˆ—è¡¨
#                     "source_url": source,
#                     # "metadata": {
#                     #     "file_type": file_type,
#                     #     "original_length": len(section)
#                     # }
#                 })
                
#         return extracted_items

#     def _perform_rerank(self, query: str, docs: List[Document], top_k: int) -> List[Document]:
#         """Cross-Encoder é‡æ’åº"""
#         if not docs: return []
#         unique_docs = {d.page_content: d for d in docs}.values()
#         docs = list(unique_docs)

#         pairs = [[query, doc.page_content] for doc in docs]
#         scores = _RERANK_MODEL.predict(pairs)
#         scored_docs = sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)
#         return [doc for score, doc in scored_docs[:top_k]]

#     def _save_final_data(self, new_data: List[Dict], session_id: str) -> str:
#         save_dir = f"workspace/{session_id}/structured_data"
#         os.makedirs(save_dir, exist_ok=True)
#         path = os.path.join(save_dir, f"combined_data_{session_id}.json")
#         with open(path, "w", encoding="utf-8") as f:
#             json.dump({"data": new_data}, f, ensure_ascii=False, indent=4)
#         return path
    # def _split_markdown_by_headers(
    # self,
    # content: str,
    # min_level: int = 1,
    # max_level: int = 3,
    # max_chars: int = 2000,
    # overlap: int = 150,
    # ) -> List[str]:
    #     """
    #     æŒ‰ Markdown ATX æ ‡é¢˜ (#, ##, ### ...) åˆ‡åˆ†ã€‚
    #     - min_level/max_level æ§åˆ¶ç”¨å“ªäº›æ ‡é¢˜ä½œä¸ºâ€œåˆ‡åˆ†ç‚¹â€
    #     - æ¯ä¸ª section å¦‚æœè¶…è¿‡ max_charsï¼Œä¼šå†åšäºŒæ¬¡åˆ‡åˆ†ï¼ˆå¸¦ overlapï¼‰
    #     """
    #     # åŒ¹é…è¡Œé¦–æ ‡é¢˜ï¼š# åˆ° ######ï¼Œåé¢è‡³å°‘ä¸€ä¸ªç©ºæ ¼
    #     header_re = re.compile(r'^(#{1,6})\s+(.+?)\s*$', re.MULTILINE)

    #     matches = list(header_re.finditer(content))
    #     if not matches:
    #         # æ²¡æ ‡é¢˜å°±æ•´ä½“è¿”å›ï¼Œåé¢äºŒæ¬¡åˆ‡åˆ†
    #         return self._chunk_text(content, max_chars=max_chars, overlap=overlap)

    #     sections: List[str] = []
    #     for i, m in enumerate(matches):
    #         level = len(m.group(1))
    #         # åªä»¥æŒ‡å®š level èŒƒå›´å†…çš„æ ‡é¢˜ä½œä¸ºåˆ‡åˆ†ç‚¹
    #         if not (min_level <= level <= max_level):
    #             continue

    #         start = m.start()
    #         # æ‰¾ä¸‹ä¸€ä¸ªâ€œå¯ç”¨æ ‡é¢˜åˆ‡åˆ†ç‚¹â€çš„ä½ç½®ä½œä¸º end
    #         end = len(content)
    #         for j in range(i + 1, len(matches)):
    #             lvl2 = len(matches[j].group(1))
    #             if min_level <= lvl2 <= max_level:
    #                 end = matches[j].start()
    #                 break

    #         block = content[start:end].strip()
    #         if block:
    #             sections.append(block)

    #     # å¦‚æœå› ä¸º level è¿‡æ»¤å¯¼è‡´ sections ä¸ºç©ºï¼Œé™çº§ï¼šæŒ‰ä»»æ„æ ‡é¢˜åˆ‡
    #     if not sections:
    #         sections = []
    #         for i, m in enumerate(matches):
    #             start = m.start()
    #             end = matches[i + 1].start() if i + 1 < len(matches) else len(content)
    #             block = content[start:end].strip()
    #             if block:
    #                 sections.append(block)

    #     # äºŒæ¬¡åˆ‡åˆ†ï¼šé¿å…æŸä¸ª section å¤ªé•¿
    #     final_sections: List[str] = []
    #     for s in sections:
    #         if len(s) > max_chars:
    #             final_sections.extend(self._chunk_text(s, max_chars=max_chars, overlap=overlap))
    #         else:
    #             final_sections.append(s)

    #     return final_sections


#     def _chunk_text(self, text: str, max_chars: int = 2000, overlap: int = 150) -> List[str]:
#         """ç®€å•æŒ‰å­—ç¬¦é•¿åº¦åˆ‡ chunkï¼ˆç”¨äº section å¤ªé•¿æ—¶çš„äºŒæ¬¡åˆ‡åˆ†ï¼‰"""
#         text = text.strip()
#         if len(text) <= max_chars:
#             return [text] if text else []

#         chunks = []
#         start = 0
#         while start < len(text):
#             end = min(len(text), start + max_chars)
#             chunk = text[start:end].strip()
#             if chunk:
#                 chunks.append(chunk)
#             if end == len(text):
#                 break
#             start = max(0, end - overlap)
#         return chunks
