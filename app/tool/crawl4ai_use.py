import os
import time
import asyncio
from typing import List, Union
from urllib.parse import urlparse

# å¼•å…¥ Crawl4AI
try:
    from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
except ImportError:
    raise ImportError("Please install crawl4ai: pip install crawl4ai")

from app.logger import logger
from app.tool.base import BaseTool, ToolResult

class Crawl4aiTool(BaseTool):
    name: str = "crawl4ai"
    description: str = """
    High-performance web crawler that processes multiple URLs in PARALLEL.
    It saves the raw markdown content to a session-specific local folder 
    and returns the list of file paths.
    """

    parameters: dict = {
        "type": "object",
        "properties": {
            "urls": {
                "type": "array",
                "items": {"type": "string"},
                "description": "List of URLs to crawl (e.g., ['http://site1.com', 'http://site2.com']).",
            }
        },
        "required": ["urls"],
    }

    async def execute(self, urls: Union[str, List[str]]) -> ToolResult:
        # 1. å‚æ•°å½’ä¸€åŒ–
        if isinstance(urls, str):
            url_list = [urls]
        else:
            url_list = urls

        if not url_list:
            return ToolResult(error="No URLs provided.")

        logger.info(f"ğŸ•·ï¸ Starting parallel crawl for {len(url_list)} URLs...")

        # =================================================================
        # 2. ä¼šè¯éš”ç¦»ï¼šåˆ›å»º Session ä¸“å±ç›®å½•
        # =================================================================
        # è·å– Session IDï¼Œå¦‚æœæ²¡æœ‰åˆ™æ”¾å…¥ default_session
        session_id = os.environ.get("MANUS_SESSION_ID", "default_session")
        
        # ç›®å½•ç»“æ„: workspace/{session_id}/raw_data/
        save_dir = os.path.join("workspace",session_id, "raw_data" ) #
        os.makedirs(save_dir, exist_ok=True)
        # =================================================================

        # 3. é…ç½® Crawl4AI
        browser_config = BrowserConfig(
            headless=True,
            verbose=False,
            java_script_enabled=True,
        )
        
        # run_config: å•æ¬¡çˆ¬å–ä»»åŠ¡çš„é…ç½®
        run_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            word_count_threshold=5,
            excluded_tags=["script", "style", "nav", "footer"], # æ’é™¤å¹²æ‰°æ ‡ç­¾
            remove_overlay_elements=True,
            process_iframes=True,
        )

        results_summary = []
        
        # 4. å¯åŠ¨çˆ¬è™«ä¸Šä¸‹æ–‡
        try:
            async with AsyncWebCrawler(config=browser_config) as crawler:
                
                # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
                tasks = []
                for url in url_list:
                    tasks.append(crawler.arun(url=url, config=run_config))
                
                # å¹¶å‘æ‰§è¡Œ
                crawl_results = await asyncio.gather(*tasks, return_exceptions=True)

                # 5. å¤„ç†ç»“æœ
                for i, result in enumerate(crawl_results):
                    url = url_list[i]

                    # å¤„ç†å¼‚å¸¸
                    if isinstance(result, Exception):
                        error_msg = f"âŒ Error crawling {url}: {str(result)}"
                        logger.error(error_msg)
                        results_summary.append(error_msg)
                        continue

                    # å¤„ç†æˆåŠŸ
                    if result.success:
                        try:
                            markdown_content = result.markdown or ""
                            
                            # =================================================================
                            # æ–°å¢é€»è¾‘ï¼šå­—æ•°æ ¡éªŒ (å°‘äº 500 å­—ç¬¦åˆ™è·³è¿‡)
                            # =================================================================
                            content_length = len(markdown_content)
                            if content_length < 500:
                                skip_msg = f"â© Skipped: {url} (Content too short: {content_length} chars)"
                                logger.info(skip_msg)
                                results_summary.append(skip_msg)
                                continue
                            # ç”Ÿæˆæ–‡ä»¶å
                            parsed = urlparse(url)
                            domain = parsed.netloc.replace("www.", "").replace(".", "_")
                            # å–è·¯å¾„çš„ä¸€éƒ¨åˆ†é˜²æ­¢æ–‡ä»¶åé‡å¤ï¼Œå¹¶é™åˆ¶é•¿åº¦
                            path_part = parsed.path.strip("/").replace("/", "_")[:50]
                            if not path_part:
                                path_part = "index"
                            
                            timestamp = int(time.time())
                            filename = f"{timestamp}_{domain}_{path_part}.md"
                            
                            # å®Œæ•´è·¯å¾„åŒ…å« Session å­ç›®å½•
                            filepath = os.path.join(save_dir, filename)

                            # å†™å…¥æ–‡ä»¶
                            content_to_save = f"<!-- Source: {url} -->\n<!-- Time: {time.ctime()} -->\n\n"
                            content_to_save += result.markdown or ""

                            with open(filepath, "w", encoding="utf-8") as f:
                                f.write(content_to_save)
                            
                            # âœ… å…³é”®ï¼šè¿”å›çš„æ–‡ä»¶è·¯å¾„æ˜¯åŒ…å« session_id çš„è·¯å¾„
                            # è¿™æ ·åç»­çš„ StructuredRetrievalTool å°±èƒ½é€šè¿‡è¿™ä¸ªè·¯å¾„æ‰¾åˆ°æ–‡ä»¶
                            success_msg = f"âœ… Success: {url} -> Saved to '{filepath}'"
                            logger.info(success_msg)
                            results_summary.append(success_msg)

                        except Exception as e:
                            logger.error(f"Failed to save file for {url}: {e}")
                            results_summary.append(f"âš ï¸ Crawled {url} but failed to save file.")
                    else:
                        fail_msg = f"âŒ Failed: {url} (Status: {getattr(result, 'status_code', 'Unknown')})"
                        logger.warning(fail_msg)
                        results_summary.append(fail_msg)

        except Exception as e:
            return ToolResult(error=f"Critical Crawler Error: {str(e)}")

        # 6. è¿”å›æ‘˜è¦ç»™ Agent
        final_output = "Batch Crawl Completed. Results:\n" + "\n".join(results_summary)
        
        return ToolResult(output=final_output)