import asyncio
import json
import os
from datetime import datetime, timezone
import time
from typing import List, Set,Dict
from pydantic import Field
from tavily import TavilyClient
from urllib.parse import urlparse
from app.logger import logger

# å¼•å…¥ä½ çš„é¡¹ç›®ä¾èµ–
from app.tool.base import BaseTool, ToolResult
from app.llm import LLM
from app.schema import Message  # ç¡®ä¿å¼•å…¥ Message ä»¥é˜²æŠ¥é”™

class TopicResearchTool(BaseTool):
    name: str = "topic_research"
    description: str = """
    Deep research tool using Tavily API.
    1. Analyzes user topic and generates 3-5 professional search queries using LLM.
    2. Executes these searches in parallel via Tavily to get high-quality results.
    3. Returns a deduplicated list of relevant URLs.
    """
    
    parameters: dict = {
        "type": "object",
        "properties": {
            "topic": {
                "type": "string",
                "description": "The user's research topic (e.g., '2025 sofa design trends')."
            },
            "max_urls": {
                "type": "integer",
                "description": "Max number of unique URLs to return.",
                "default": 15
            }
        },
        "required": ["topic"]
    }

    llm: LLM = Field(default_factory=LLM, exclude=True)
    
    _tavily_client: TavilyClient = None

    def __init__(self, **data):
        super().__init__(**data)
        api_key = os.getenv("TAVILY_API_KEY") 
        self._tavily_client = TavilyClient(api_key=api_key)

    async def execute(self, topic: str, max_urls: int = 20) -> ToolResult:
        logger.info(f"ğŸ§  Brainstorming search queries for: '{topic}'")

        # 1. ä½¿ç”¨ LLM ç”Ÿæˆ 3-5 ä¸ªå¤šç»´åº¦æœç´¢è¯
        queries = await self._generate_smart_queries(topic)
        
        if not queries:
            logger.warning("LLM failed to generate queries, falling back to simple search.")
            queries = [topic, f"{topic} trends 2025"]

        logger.info(f"ğŸ” Executing Tavily searches for: {queries}")

        # 2. å¹¶è¡Œæ‰§è¡Œæœç´¢ (Tavily Search)
        # Tavily SDK æ˜¯åŒæ­¥çš„ï¼Œæˆ‘ä»¬éœ€è¦ç”¨ asyncio.to_thread æŠŠå®ƒå˜æˆå¼‚æ­¥éé˜»å¡ï¼Œå¦åˆ™ä¼šå¡ä½ Agent
        search_tasks = []
        for q in queries:
            search_tasks.append(self._perform_tavily_search(q))
        

        # ç­‰å¾…æ‰€æœ‰æœç´¢å®Œæˆ
        search_results_list = await asyncio.gather(*search_tasks, return_exceptions=True)
       
        # 3. ç»“æœèšåˆä¸å»é‡
        seen_urls: Set[str] = set()
        final_results = []
        
        for batch_results in search_results_list:
            if isinstance(batch_results, Exception):
                logger.error(f"A search task failed: {batch_results}")
                continue
            
            # batch_results æ˜¯ Tavily è¿”å›çš„ 'results' åˆ—è¡¨
            for item in batch_results:
                url = item.get('url')
                title = item.get('title', 'No Title')
                
                if url and url not in seen_urls:
                    if self._is_valid_url(url):
                        seen_urls.add(url)
                        # æš‚æ—¶åªå­˜ URLï¼Œå¦‚æœ Agent éœ€è¦ Title å¯ä»¥æŠŠè¿™é‡Œæ”¹æˆ dict
                        final_results.append(url)

        # 4. æˆªæ–­ç»“æœ
        selected_urls = final_results[:max_urls]
        logger.info(f"total length:{len(final_results)} âœ… Found {len(selected_urls)} unique high-quality URLs.")

        # è¿”å› JSON æ ¼å¼çš„ URL åˆ—è¡¨
        return ToolResult(output=json.dumps(selected_urls))

    async def _perform_tavily_search(self, query: str) -> List[dict]:
        """
        åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œ Tavily æœç´¢ï¼Œé¿å…é˜»å¡å¼‚æ­¥å¾ªç¯
        æ¯ä¸ªå…³é”®è¯å›ºå®šè¿”å›5ä¸ªé«˜è´¨é‡URLï¼Œä¿è¯ç»“æœæ•°é‡ä¸”åç»­æ˜“å»é‡
        """
        def search_sync():
            try:
                response = self._tavily_client.search(
                    query=query,
                    search_depth="advanced",  # ä½¿ç”¨é«˜çº§æœç´¢æ·±åº¦
                    max_results=5,            # æ¯ä¸ªqueryä»…å–å‰5ä¸ªï¼ˆç²¾å‡†æ§åˆ¶æ•°é‡ï¼‰
                    include_answer=False,
                    include_raw_content=False,
                    include_images=False,
                    # å¯é€‰ï¼šæ·»åŠ æ—¶é—´èŒƒå›´ï¼ŒåŒ¹é…ä½ ä¹‹å‰çš„æ—¶é—´æˆ³éœ€æ±‚
                    # start_date=self.search_start_date,
                    # end_date=self.search_end_date,
                    # include_domains=[
                    #     "taobao.com", "tmall.com", "ikea.com",
                    #     "wgsn.com", "minotti.com", "xiaohongshu.com", "tiktok.com"
                    # ],
                )
                # å¯¹å•queryçš„3ä¸ªç»“æœåšåŸºç¡€æ¸…æ´—ï¼ˆè¿‡æ»¤æ— æ•ˆURLï¼‰
                raw_results = response.get('results', [])
                cleaned_results = []
                for res in raw_results:
                    url = res.get('url', '').strip()
                    if url and url.startswith(('http://', 'https://')):  # è¿‡æ»¤æ— æ•ˆURL
                        cleaned_results.append(res)
                # ä¸è¶³3ä¸ªæ—¶è¡¥å……ç©ºç»“æœï¼ˆä¿è¯æ•°é‡ï¼Œåç»­æ±‡æ€»æ—¶è‡ªåŠ¨è¿‡æ»¤ï¼‰
                return cleaned_results[:3]
            except Exception as e:
                logger.error(f"Tavily search error for query '{query}': {e}")
                return []

        # ä½¿ç”¨ asyncio.to_thread (Python 3.9+)
        return await asyncio.to_thread(search_sync)
    async def _generate_smart_queries(self, topic: str, timestamp: float = None) -> List[str]:
        """
        è®© LLM ç”Ÿæˆ 3-5 ä¸ªé«˜è´¨é‡æœç´¢è¯ï¼ˆå¸¦æ—¶é—´æˆ³ï¼Œç¡®ä¿æœç´¢è¯æ—¶æ•ˆæ€§ï¼‰
        :param topic: æ ¸å¿ƒè°ƒç ”ä¸»é¢˜
        :param timestamp: æ—¶é—´æˆ³ï¼ˆç§’çº§ï¼‰ï¼Œè‹¥ä¸ä¼ åˆ™ä½¿ç”¨å½“å‰æ—¶é—´
        :return: 3-5ä¸ªæ—¶æ•ˆæ€§æœç´¢è¯åˆ—è¡¨
        """
        # å¤„ç†æ—¶é—´æˆ³ï¼šé»˜è®¤ç”¨å½“å‰æ—¶é—´ï¼Œè½¬æ¢ä¸ºæ˜“è¯»çš„å¹´ä»½/å¹´æœˆæ ¼å¼
        if timestamp is None:
            timestamp = time.time()
        # è½¬æ¢æ—¶é—´æˆ³ä¸º "YYYY" å’Œ "YYYY-MM" æ ¼å¼ï¼ˆé€‚é…æœç´¢è¯åœºæ™¯ï¼‰
        search_year = datetime.fromtimestamp(timestamp).strftime("%Y")
        search_month = datetime.fromtimestamp(timestamp).strftime("%Y-%m")

        prompt = f"""
        You are an expert Market Researcher and SEO Specialist.
        Your goal is to generate **3 to 5 highly distinct** search queries to maximize information coverage for the topic: "{topic}".
        
        **Time Context:**
        - Research Target Time: {search_month} (Year: {search_year})
        - Timestamp: {int(timestamp)}
        - **Constraint:** ALL queries must explicitly include time markers like "{search_year}", "{search_month}", or "Q{int((datetime.now().month-1)/3)+1} {search_year}".

        **Strategic Dimensions (Generate distinct queries for each dimension):**
        1.  **Quantitative/Sales Data:** Bestseller lists, market share statistics, sales volume rankings (e.g., "top selling mid-range sofas {search_year} statistics").
        2.  **Qualitative/Design Trends:** Aesthetic evolution, colors, materials, shapes (e.g., "trending sofa fabric types {search_year}", "living room furniture color trends {search_year}").
        3.  **Industry Authority:** Professional forecasts, trade shows (e.g., Milan Design Week), wgsn reports (e.g., "furniture industry market analysis report {search_year}").
        4.  **Platform/Competitor Specific:** Specific retailer data (e.g., "IKEA vs Wayfair sofa sales {search_year}", "Amazon furniture best sellers {search_month}").

        **Strict Requirements:**
        - **Maximize Semantic Distance:** Do NOT generate synonymous queries (e.g., do not output both "best sofas" and "top rated sofas").
        - **Focus on Diversity:** Ensure the list covers at least 4 of the 5 dimensions above.
        - Return ONLY a raw JSON list of strings. No markdown formatting.
        - Example Output: ["{search_year} mid-range sofa market share", "trending velvet sofa colors {search_year}", "best sofa for back pain reviews {search_year}", "IKEA 2025 catalog living room", "sofa industry supply chain trends {search_year}"]
        """
        
        try:
            # æ„é€  Message å¯¹è±¡
            messages = [
                Message(role="user", content=prompt)
            ]
            response = await self.llm.ask(messages)
            
            # æ¸…æ´—å“åº”å†…å®¹
            cleaned_response = response.replace("```json", "").replace("```", "").strip()
            queries = json.loads(cleaned_response)
            
            return queries
        except Exception as e:
            logger.error(f"Error generating queries with LLM: {e}")
            # å¼‚å¸¸æ—¶è¿”å›å¸¦æ—¶é—´çš„å…œåº•æœç´¢è¯
            return [f"{search_year} {topic}"]

    def _is_valid_url(self, url: str) -> bool:
        """ç®€å•çš„ URL è¿‡æ»¤å™¨"""
        # æ’é™¤æ–‡ä»¶ç±»å‹
        skip_extensions = ('.pdf', '.doc', '.docx', '.ppt', '.pptx', '.xml', '.json', '.jpg', '.png')
       
        url_lower = url.lower()
        if url_lower.endswith(skip_extensions):
            return False
        return True
    