import asyncio
from typing import List, Dict, Any, Union
from app.tool.base import BaseTool, ToolResult
from app.tool.crawl4ai import Crawl4aiTool
from app.tool.browser_use_tool import BrowserUseTool
from app.logger import logger

class SmartScraperTool(BaseTool):
    """
    A smart scraper that attempts to fetch content using a waterfall strategy:
    1. Try Crawl4AI (Fast, Headerless)
    2. If failed/empty, fallback to BrowserUse (Full Browser, Slow, Anti-detection)
    """
    name: str = "smart_scraper"
    description: str = """
    Intelligently scrapes content from a list of URLs.
    It first tries a fast crawler. If the website blocks it or returns empty content, 
    it automatically falls back to a full browser simulation to extract the data.
    Use this for reading blog posts, news articles, or documentation.
    """
    
    parameters: dict = {
        "type": "object",
        "properties": {
            "urls": {
                "type": "array",
                "items": {"type": "string"},
                "description": "List of URLs to scrape.",
            },
            "instruction": {
                "type": "string",
                "description": "Specific instruction for what to extract (used if fallback to browser is needed).",
                "default": "Extract the main content, including title, body text, and key images."
            }
        },
        "required": ["urls"]
    }

    # å†…éƒ¨æŒæœ‰ä¸¤ä¸ªå·¥å…·
    crawler: Crawl4aiTool = Crawl4aiTool()
    browser_tool: BrowserUseTool = BrowserUseTool()

    async def execute(self, urls: List[str], instruction: str = "Extract main content") -> ToolResult:
        results = []
        
        for url in urls:
            logger.info(f"ğŸš€ SmartScraper processing: {url}")
            
            # --- é˜¶æ®µ 1: å°è¯• Crawl4AI (å¿«) ---
            scrape_success = False
            content_data = None
            
            try:
                # ä½¿ç”¨ bypass_cache=True ç¡®ä¿æ‹¿åˆ°æœ€æ–°æ•°æ®
                crawl_result = await self.crawler.execute(urls=[url], bypass_cache=True)
                
                # æ£€æŸ¥çˆ¬å–ç»“æœæ˜¯å¦æœ‰æ•ˆ
                # Crawl4AI è¿”å›çš„æ˜¯æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬éœ€è¦åˆ¤æ–­é‡Œé¢æ˜¯å¦åŒ…å«æœ‰æ•ˆä¿¡æ¯
                # è¿™é‡Œåšä¸€ä¸ªç®€å•çš„å¯å‘å¼åˆ¤æ–­ï¼šå¦‚æœæ²¡æœ‰ Markdown å†…å®¹æˆ–è€…å†…å®¹å¤ªçŸ­ï¼Œè§†ä¸ºå¤±è´¥
                if "Markdown: None" not in crawl_result.output and "Success (HTTP 200)" in crawl_result.output:
                    # è¿›ä¸€æ­¥æ£€æŸ¥å†…å®¹é•¿åº¦
                    # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æ£€æŸ¥ï¼Œå®é™…å¯ä»¥è§£æ output å­—ç¬¦ä¸²
                    if len(crawl_result.output) > 500: 
                        logger.info(f"âœ… Crawl4AI success for {url}")
                        results.append(f"Source: {url} (via Crawler)\n{crawl_result.output}")
                        scrape_success = True
                    else:
                        logger.warning(f"âš ï¸ Crawl4AI returned too little data for {url}")
                else:
                    logger.warning(f"âš ï¸ Crawl4AI failed status check for {url}")
                    
            except Exception as e:
                logger.error(f"âŒ Crawl4AI error for {url}: {e}")

            # --- é˜¶æ®µ 2: é™çº§åˆ° BrowserUse (æ…¢ä½†ç¨³) ---
            if not scrape_success:
                logger.info(f"ğŸ”„ Falling back to BrowserUse for {url}")
                try:
                    # 1. å¯¼èˆª
                    await self.browser_tool.execute(action="go_to_url", url=url)
                    
                    # 2. æå– (ä½¿ç”¨æˆ‘ä»¬ä¼˜åŒ–è¿‡çš„æ”¯æŒå¤šæ¨¡æ€çš„æå–é€»è¾‘)
                    extract_result = await self.browser_tool.execute(
                        action="extract_content", 
                        goal=instruction
                    )
                    
                    if not extract_result.error:
                        results.append(f"Source: {url} (via Browser)\n{extract_result.output}")
                        logger.info(f"âœ… BrowserUse success for {url}")
                    else:
                        results.append(f"âŒ Failed to scrape {url}: {extract_result.error}")
                        
                except Exception as e:
                    logger.error(f"âŒ BrowserUse error for {url}: {e}")
                    results.append(f"âŒ Critical error scraping {url}: {str(e)}")

        return ToolResult(output="\n\n".join(results))